{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d96b285-3467-49e6-a7d7-1b8dd6d1c28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 16:31:20.875832: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-26 16:31:20.903478: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-26 16:31:20.903522: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-26 16:31:20.922103: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-26 16:31:21.856188: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9239237f-9509-4f14-b1ad-a2e98f605d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INICIANDO ETAPA 1: PREPARAÇÃO E EMPACOTAMENTO ---\n",
      "Baixando modelo de s3://sagemaker-us-east-2-982534382598/lstm-stock-prediction-artifacts/lstm_stock_prediction_model.h5...\n",
      "Baixando scaler de s3://sagemaker-us-east-2-982534382598/lstm-stock-prediction-artifacts/scaler.joblib...\n",
      "Downloads concluídos.\n",
      "Scripts de inferência e dependências criados.\n",
      "Convertendo 'modelo_local.h5' para o formato SavedModel...\n",
      "INFO:tensorflow:Assets written to: export/Servo/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: export/Servo/1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo convertido e scaler posicionado.\n",
      "Criando o arquivo 'model.tar.gz'...\n",
      "Fazendo upload para s3://sagemaker-us-east-2-982534382598/sagemaker-lstm-artifacts/previsao-acoes/model.tar.gz...\n",
      "--- ETAPA 1 CONCLUÍDA COM SUCESSO ---\n",
      "\n",
      "--- INICIANDO ETAPA 2: DEPLOY DO ENDPOINT ---\n",
      "Iniciando o deploy do endpoint. Isso pode levar vários minutos...\n",
      "-----!\n",
      "✅ Deploy concluído com sucesso!\n",
      "Nome do Endpoint: tensorflow-inference-2025-07-26-19-17-42-056\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import tensorflow as tf\n",
    "import tarfile\n",
    "import os\n",
    "import sagemaker\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "# ==============================================================================\n",
    "# ETAPA 1: PREPARAÇÃO E EMPACOTAMENTO\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"--- INICIANDO ETAPA 1: PREPARAÇÃO E EMPACOTAMENTO ---\")\n",
    "\n",
    "# --- 1. CONFIGURAÇÃO INICIAL ---\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# --- CONFIGURE OS CAMINHOS DO SEU BUCKET S3 AQUI ---\n",
    "# Substitua com o nome do seu bucket e os caminhos corretos\n",
    "s3_bucket_name = 'sagemaker-us-east-2-982534382598'\n",
    "\n",
    "# Caminhos dos artefatos de ENTRADA no S3\n",
    "input_model_s3_key = 'lstm-stock-prediction-artifacts/lstm_stock_prediction_model.h5'      # SUBSTITUA\n",
    "input_scaler_s3_key = 'lstm-stock-prediction-artifacts/scaler.joblib' # SUBSTITUA\n",
    "\n",
    "# Caminho de SAÍDA no S3 onde o artefato final será salvo\n",
    "output_s3_key_prefix = 'sagemaker-lstm-artifacts/previsao-acoes'\n",
    "\n",
    "# Nomes dos arquivos locais temporários\n",
    "local_h5_path = 'modelo_local.h5'\n",
    "local_scaler_path = 'scaler_local.joblib'\n",
    "\n",
    "# Estrutura de diretórios para empacotamento\n",
    "export_dir = 'export'\n",
    "saved_model_path = os.path.join(export_dir, 'Servo/1')\n",
    "code_dir = os.path.join(export_dir, 'code')\n",
    "\n",
    "# Limpa execuções anteriores para garantir um ambiente limpo\n",
    "if os.path.exists(export_dir): shutil.rmtree(export_dir)\n",
    "if os.path.exists('model.tar.gz'): os.remove('model.tar.gz')\n",
    "os.makedirs(saved_model_path, exist_ok=True)\n",
    "os.makedirs(code_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- 2. BAIXAR MODELO E SCALER DO S3 ---\n",
    "print(f\"Baixando modelo de s3://{s3_bucket_name}/{input_model_s3_key}...\")\n",
    "s3_client.download_file(s3_bucket_name, input_model_s3_key, local_h5_path)\n",
    "\n",
    "print(f\"Baixando scaler de s3://{s3_bucket_name}/{input_scaler_s3_key}...\")\n",
    "s3_client.download_file(s3_bucket_name, input_scaler_s3_key, local_scaler_path)\n",
    "print(\"Downloads concluídos.\")\n",
    "\n",
    "\n",
    "# --- 3. CRIAR 'inference.py' E 'requirements.txt' ---\n",
    "# Criando o requirements.txt para instalar dependências\n",
    "requirements_path = os.path.join(code_dir, 'requirements.txt')\n",
    "with open(requirements_path, 'w') as f:\n",
    "    f.write('scikit-learn\\njoblib')\n",
    "\n",
    "# Conteúdo do inference.py para o modelo LSTM\n",
    "inference_code = \"\"\"\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "scaler = None\n",
    "\n",
    "def _load_scaler():\n",
    "    global scaler\n",
    "    if scaler is None:\n",
    "        print(\"Scaler não está em memória. Carregando de 'scaler.joblib'...\")\n",
    "        scaler_path = os.path.join('/opt/ml/model/code', 'scaler_local.joblib')\n",
    "        if os.path.exists(scaler_path):\n",
    "            scaler = joblib.load(scaler_path)\n",
    "            print(\"Scaler carregado com sucesso.\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"ERRO CRÍTICO: 'scaler.joblib' não encontrado em {scaler_path}\")\n",
    "\n",
    "def input_handler(data, context):\n",
    "    print(\"Executando input_handler...\")\n",
    "    _load_scaler()\n",
    "\n",
    "    input_data = json.loads(data.read().decode('utf-8'))['instances']\n",
    "    # input_array terá o shape (1, 60)\n",
    "    input_array = np.array(input_data, dtype=np.float32)\n",
    "    input_reshaped_for_scaler = input_array.reshape(-1, 1)\n",
    "\n",
    "    # Agora o scaler recebe os dados no formato esperado (60 amostras, 1 característica).\n",
    "    scaled_input = scaler.transform(input_reshaped_for_scaler) # O resultado terá o shape (60, 1)\n",
    "\n",
    "    # Remodelar os dados normalizados para o formato que o modelo LSTM espera: (1, 60, 1)\n",
    "    # Formato: [batch_size, timesteps, features]\n",
    "    reshaped_for_model = scaled_input.reshape(1, -1, 1)\n",
    "\n",
    "    return json.dumps({\"instances\": reshaped_for_model.tolist()})\n",
    "\n",
    "def output_handler(response, context):\n",
    "    print(\"Executando output_handler...\")\n",
    "    _load_scaler()\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"Erro do servidor de modelos: {response.content.decode('utf-8')}\")\n",
    "\n",
    "    tfs_response = json.loads(response.content.decode('utf-8'))\n",
    "    \n",
    "    if 'predictions' not in tfs_response:\n",
    "        raise ValueError(f\"Resposta do modelo não contém a chave 'predictions': {tfs_response}\")\n",
    "        \n",
    "    scaled_prediction = np.array(tfs_response['predictions'])\n",
    "\n",
    "    # A predição do modelo (shape 1,1) já está no formato correto para inverse_transform.\n",
    "    real_prediction = scaler.inverse_transform(scaled_prediction)\n",
    "\n",
    "    result = {\"predicted_price\": real_prediction.tolist()[0][0]}\n",
    "    \n",
    "    # --- CORREÇÃO APLICADA AQUI ---\n",
    "    # Retornando uma tupla com a resposta e o content-type, como o serviço espera.\n",
    "    return json.dumps(result), context.accept_header\n",
    "\"\"\"\n",
    "inference_script_path = os.path.join(code_dir, 'inference.py')\n",
    "with open(inference_script_path, 'w') as f:\n",
    "    f.write(inference_code)\n",
    "print(\"Scripts de inferência e dependências criados.\")\n",
    "\n",
    "\n",
    "# --- 4. CONVERTER MODELO E ORGANIZAR ARTEFATOS ---\n",
    "print(f\"Convertendo '{local_h5_path}' para o formato SavedModel...\")\n",
    "model = tf.keras.models.load_model(local_h5_path)\n",
    "tf.saved_model.save(model, saved_model_path)\n",
    "\n",
    "# Copiar o scaler baixado para o diretório de código\n",
    "shutil.copy(local_scaler_path, code_dir)\n",
    "print(\"Modelo convertido e scaler posicionado.\")\n",
    "\n",
    "\n",
    "# --- 5. EMPACOTAR E FAZER UPLOAD ---\n",
    "output_tar_path = 'model.tar.gz'\n",
    "print(f\"Criando o arquivo '{output_tar_path}'...\")\n",
    "with tarfile.open(output_tar_path, 'w:gz') as tar:\n",
    "    tar.add(export_dir, arcname='.')\n",
    "\n",
    "output_model_path_s3 = f\"s3://{s3_bucket_name}/{output_s3_key_prefix}/model.tar.gz\"\n",
    "print(f\"Fazendo upload para {output_model_path_s3}...\")\n",
    "sagemaker_session.upload_data(path=output_tar_path, key_prefix=output_s3_key_prefix)\n",
    "print(\"--- ETAPA 1 CONCLUÍDA COM SUCESSO ---\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ETAPA 2: DEPLOY DO ENDPOINT\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n--- INICIANDO ETAPA 2: DEPLOY DO ENDPOINT ---\")\n",
    "\n",
    "# --- 1. CRIAR O OBJETO DO MODELO ---\n",
    "sagemaker_model = TensorFlowModel(\n",
    "    model_data=output_model_path_s3,\n",
    "    role=role,\n",
    "    framework_version='2.12', # Use a mesma versão principal do TF usada no treinamento\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    entry_point='inference.py'\n",
    ")\n",
    "\n",
    "# --- 2. IMPLANTAR O MODELO ---\n",
    "print(\"Iniciando o deploy do endpoint. Isso pode levar vários minutos...\")\n",
    "predictor = sagemaker_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium'\n",
    ")\n",
    "print(f\"\\n✅ Deploy concluído com sucesso!\")\n",
    "print(f\"Nome do Endpoint: {predictor.endpoint_name}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5146a697-2bbc-4fec-a3c3-8383175c5be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26911/682262794.py:7: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(ACAO, start=START_DATE, end=END_DATE)\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "ACAO = 'ITUB'            # Símbolo da empresa (ex: 'DIS' para Disney)\n",
    "START_DATE = '2025-05-25'  # Data de início da coleta de dados\n",
    "END_DATE = '2025-07-24'    # Data de fim da coleta de dados\n",
    "SEQUENCE = 60       # Número de dias passados para prever o próximo dia (timestep do LSTM)\n",
    "\n",
    "df = yf.download(ACAO, start=START_DATE, end=END_DATE)\n",
    "\n",
    "test_60 = [ i[0] for i in df['Close'].values.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b77bed2f-b1df-4ad6-bdd9-fd6d04bbf401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enviando dados de teste para o endpoint...\n",
      "\n",
      "Predição recebida do endpoint:\n",
      "{'predicted_price': 6.31754140722038}\n"
     ]
    }
   ],
   "source": [
    "# --- 3. TESTAR O ENDPOINT ---\n",
    "print(\"\\nEnviando dados de teste para o endpoint...\")\n",
    "sequence_length = 60 # Ajuste para o tamanho da sequência do seu modelo\n",
    "test_sequence = np.random.uniform(low=100.0, high=150.0, size=(1, sequence_length)).tolist()\n",
    "\n",
    "payload = {\"instances\": test_60}\n",
    "\n",
    "try:\n",
    "    response = predictor.predict(payload)\n",
    "    print(\"\\nPredição recebida do endpoint:\")\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Ocorreu um erro ao invocar o endpoint: {e}\")\n",
    "\n",
    "# --- 4. LIMPEZA (OPCIONAL) ---\n",
    "# print(\"\\nPara deletar o endpoint, remova o comentário da linha abaixo e execute novamente:\")\n",
    "# predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b828b1a-16a6-4527-961c-b48fc3309d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.730000019073486,\n",
       " 6.639999866485596,\n",
       " 6.630000114440918,\n",
       " 6.590000152587891,\n",
       " 6.539999961853027,\n",
       " 6.590000152587891,\n",
       " 6.53000020980835,\n",
       " 6.519999980926514,\n",
       " 6.559999942779541,\n",
       " 6.559999942779541,\n",
       " 6.539999961853027,\n",
       " 6.579999923706055,\n",
       " 6.610000133514404,\n",
       " 6.559999942779541,\n",
       " 6.739999771118164,\n",
       " 6.739999771118164,\n",
       " 6.679999828338623,\n",
       " 6.630000114440918,\n",
       " 6.650000095367432,\n",
       " 6.789999961853027,\n",
       " 6.590000152587891,\n",
       " 6.599999904632568,\n",
       " 6.619999885559082,\n",
       " 6.789999961853027,\n",
       " 6.789999961853027,\n",
       " 6.809999942779541,\n",
       " 6.920000076293945,\n",
       " 6.840000152587891,\n",
       " 6.809999942779541,\n",
       " 6.650000095367432,\n",
       " 6.369999885559082,\n",
       " 6.300000190734863,\n",
       " 6.269999980926514,\n",
       " 6.289999961853027,\n",
       " 6.329999923706055,\n",
       " 6.449999809265137,\n",
       " 6.300000190734863,\n",
       " 6.389999866485596,\n",
       " 6.309999942779541,\n",
       " 6.449999809265137]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71998227-b40a-4df2-8f33-34d5957385f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
